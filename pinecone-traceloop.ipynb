{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to trace-able RAG with Traceloop and Pinecone\n",
        "\n",
        "This Notebook demonstrates how to configure tracing and monitoring for your RAG pipeline with Traceloop.\n",
        "\n",
        "It demonstrates:\n",
        "\n",
        "*   Configuring Traceloop to observe your RAG pipeline\n",
        "*   Creating a simple RAG pipeline using LangChain and Pinecone\n",
        "*   Altering on low relevance scores from Pinecone when there is no good context to return for a given query\n"
      ],
      "metadata": {
        "id": "6UOKXAb2qUIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Step 1: Install dependencies (the Python packages that our program uses)"
      ],
      "metadata": {
        "id": "iW6MnBhRnG-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \\\n",
        "    langchain==0.1.20 \\\n",
        "    openai \\\n",
        "    datasets==2.10.1 \\\n",
        "    pinecone-client \\\n",
        "    tiktoken \\\n",
        "    traceloop-sdk==0.19.0 \\\n",
        "    langchain_openai \\\n",
        "    langchain_pinecone"
      ],
      "metadata": {
        "id": "bPPRoNs7pJnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare target data\n",
        "\n",
        "Step 2. Load a dataset that contains arxiv papers about Llama2. This data could really be anything, including your private company data."
      ],
      "metadata": {
        "id": "QGgWHcOmhxE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "WxLUQeS-pYXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure an embeddings model\n",
        "\n",
        "Step 3. Set up OpenAI embeddings, using the text-embedding-3-small (1536 dimensions)"
      ],
      "metadata": {
        "id": "nuGuQwKHiqeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "# Set our secret OPENAI_API_KEY to an environment variable of the same name\n",
        "# Which the OpenAIEmbeddings class expects to find\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "5wiWyFgNs7zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enable Traceloop\n",
        "\n",
        "Step 4. Set up Traceloop for tracing and monitoring"
      ],
      "metadata": {
        "id": "Bl_fEUQTjKHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from traceloop.sdk import Traceloop\n",
        "from traceloop.sdk.instruments import Instruments\n",
        "\n",
        "Traceloop.init(api_key=userdata.get('TRACELOOP_API_KEY'))"
      ],
      "metadata": {
        "id": "wyn9o1y1vGeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Pinecone\n",
        "\n",
        "Step 5. Set up connection to Pinecone, using the latest Serverless offering, which means you don't need to worry about specifying your workload or storage needs upfront."
      ],
      "metadata": {
        "id": "Cq5zVaCvi2LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "from google.colab import userdata\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "# Use a Pinecone serverless index for effortless scaling\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "a4WN200kpaZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Pinecone serverless index\n",
        "\n",
        "Step 6. Check if a Pinecone index with our desired name already exists, and create it if it does not"
      ],
      "metadata": {
        "id": "SaZrsNhIi-tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = 'traceloop-rag'\n",
        "existing_indexes = [\n",
        "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
        "]\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in existing_indexes:\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of text-embedding-3-small\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "id": "ZuvbT9bXqLOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data ingest\n",
        "\n",
        "Step 7. Loop through our dataset, convert each chunk to vectors, and upsert the vectors alongside metadata"
      ],
      "metadata": {
        "id": "Coh6J4n-inSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm  # for progress bar\n",
        "\n",
        "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    i_end = min(len(data), i+batch_size)\n",
        "    # get batch of data\n",
        "    batch = data.iloc[i:i_end]\n",
        "    # generate unique ids for each chunk\n",
        "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
        "    # get text to embed\n",
        "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
        "    # embed text\n",
        "    embeds = embed_model.embed_documents(texts)\n",
        "    # get metadata to store in Pinecone\n",
        "    metadata = [\n",
        "        {'text': x['chunk'],\n",
        "         'source': x['source'],\n",
        "         'title': x['title']} for i, x in batch.iterrows()\n",
        "    ]\n",
        "    # add to Pinecone\n",
        "    index.upsert(vectors=zip(ids, embeds, metadata))"
      ],
      "metadata": {
        "id": "onJcUcIinRHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Pinecone via LangChain\n",
        "\n",
        "Step 8. Set up a LangChain vectorstore using Pinecone"
      ],
      "metadata": {
        "id": "qnYrAy0NH-pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "text_field = \"text\"  # the metadata field that contains our text\n",
        "\n",
        "# initialize the vector store object\n",
        "vectorstore = PineconeVectorStore(\n",
        "    index, embed_model, text_field\n",
        ")"
      ],
      "metadata": {
        "id": "O0r2FrqUuhr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement a trace-able RAG pipeline with Traceloop\n",
        "\n",
        "Step 9. Create a simple RAG chain, using Traceloop's decorator, which produces a trace-able system we can use to ask questions of our knowledgebase."
      ],
      "metadata": {
        "id": "XGsn9asKIGMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from traceloop.sdk.decorators import workflow\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "@workflow(name=\"rag_backed_query\")\n",
        "def rag_backed_query(query: str):\n",
        "  # completion llm\n",
        "  llm = ChatOpenAI(\n",
        "      openai_api_key=OPENAI_API_KEY,\n",
        "      model_name='gpt-4o',\n",
        "      temperature=0.0\n",
        "  )\n",
        "  qa = RetrievalQA.from_chain_type(\n",
        "      llm=llm,\n",
        "      chain_type=\"stuff\",\n",
        "      retriever=vectorstore.as_retriever()\n",
        "  )\n",
        "  return qa.invoke(query)"
      ],
      "metadata": {
        "id": "ztdRZPYqsmjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstrate similarity search\n",
        "\n",
        "Step 10. We can issue a similarity search directly against our vectorstore / knoweldgebase by issuing a query to our LangChain vectorstore.\n"
      ],
      "metadata": {
        "id": "52VyWDuKv80s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is so special about Llama 2?\"\n",
        "\n",
        "res = rag_backed_query(query=query)\n",
        "print(res[\"result\"])"
      ],
      "metadata": {
        "id": "8NDTfM_9nFFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observability is critical to RAG pipelines\n",
        "\n",
        "Let's now simulate a query that our RAG pipeline handles poorly, by asking it something that doesn't exist in our knowledgebase / vectorstore and that the OpenAI foundation model (Chat GPT 4o) doesn't already know well.\n",
        "\n",
        "This will demonstrate how Traceloop is able to observe and filter on low relevance scores returned by Pinecone's vector database."
      ],
      "metadata": {
        "id": "nnuWSabVqDjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you explain how Gemini works?\"\n",
        "\n",
        "res = rag_backed_query(query=query)\n",
        "print(res[\"result\"])"
      ],
      "metadata": {
        "id": "hion42a5zYgM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}